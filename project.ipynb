{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (18.0.0)\n",
      "Requirement already satisfied: fastparquet in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (2024.11.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from fastparquet) (2.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from fastparquet) (2024.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SabrinaLopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/KAIST-IC-LAB721/SDCNL/data/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from huggingface_hub) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Need help</td>\n",
       "      <td>Hi I don't really know how to phrase this situ...</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feeling so overwhelmed and hopeless</td>\n",
       "      <td>i have been so depressed these past couple wee...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicidal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nothing matters anymore, getting worse</td>\n",
       "      <td>Hi..I don't know where else to go. I am devast...</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoâ€™s tired of hearing bullshit</td>\n",
       "      <td>The shit like â€œit will get better, everyone is...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicidal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I wish I was someone else.</td>\n",
       "      <td>I wish I was prettier. I wish I didnâ€™t feel li...</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>think its over</td>\n",
       "      <td>i just donâ€™t wanna live anymore so yeah</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>To all of those feeling isolated and suffering...</td>\n",
       "      <td>Iâ€™ve learned that life is fucking sad sometime...</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>I just really wish I had died the first time I...</td>\n",
       "      <td>That's all. Nothing has gotten better and I've...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicidal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>I feel unimportant.</td>\n",
       "      <td>Not the first time I'm going through this of c...</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>Iâ€™m now genuinely thinking life for others wou...</td>\n",
       "      <td>Fight with my fiancÃ©e. Potential for her endin...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicidal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1895 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                                             Need help   \n",
       "1                   feeling so overwhelmed and hopeless   \n",
       "2                Nothing matters anymore, getting worse   \n",
       "3                       Whoâ€™s tired of hearing bullshit   \n",
       "4                            I wish I was someone else.   \n",
       "...                                                 ...   \n",
       "1890                                     think its over   \n",
       "1891  To all of those feeling isolated and suffering...   \n",
       "1892  I just really wish I had died the first time I...   \n",
       "1893                                I feel unimportant.   \n",
       "1894  Iâ€™m now genuinely thinking life for others wou...   \n",
       "\n",
       "                                                   text  label  label_text  \n",
       "0     Hi I don't really know how to phrase this situ...      0  depression  \n",
       "1     i have been so depressed these past couple wee...      1    suicidal  \n",
       "2     Hi..I don't know where else to go. I am devast...      0  depression  \n",
       "3     The shit like â€œit will get better, everyone is...      1    suicidal  \n",
       "4     I wish I was prettier. I wish I didnâ€™t feel li...      0  depression  \n",
       "...                                                 ...    ...         ...  \n",
       "1890            i just donâ€™t wanna live anymore so yeah      0  depression  \n",
       "1891  Iâ€™ve learned that life is fucking sad sometime...      0  depression  \n",
       "1892  That's all. Nothing has gotten better and I've...      1    suicidal  \n",
       "1893  Not the first time I'm going through this of c...      0  depression  \n",
       "1894  Fight with my fiancÃ©e. Potential for her endin...      1    suicidal  \n",
       "\n",
       "[1895 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Hi I don't really know how to phrase this situ...\n",
       "1    i have been so depressed these past couple wee...\n",
       "2    Hi..I don't know where else to go. I am devast...\n",
       "3    The shit like â€œit will get better, everyone is...\n",
       "4    I wish I was prettier. I wish I didnâ€™t feel li...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, str in enumerate(df['text']):\n",
    "    # print(string)\n",
    "\n",
    "    clean_str = list([val for val in str if val.isalnum() or val == ' ' or val == '.' or val == ',' or val == '?' or val == '!'])\n",
    "    clean_str = ''.join(clean_str)\n",
    "    low_clean_str = clean_str.lower()\n",
    "\n",
    "    # print(low_clean_str)\n",
    "\n",
    "    df = df.replace(df['text'][idx], low_clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi i dont really know how to phrase this situation but ill try. my life is at a really good point right now, im never really depressed over stuff and 99 percent of the time my mind is clear. im about to graduate high school and im really excited. however, people in my family and friend group have tons of issues. wether they sleep all day, hate themselves, or have no ambition to keep living on in this world, theyve got problems. i dont wanna sound like im gloating, but usually im the person that a lot of these people end up going to because usually im able to talk people through issues and help them in the long run. yeah sometimes their issues make me really sad and stuff because who doesnt feel sad when people are telling you they feel worthless.but today one of my best friends showed me that he was cutting and it really effected me. i talked to him about it and its mostly because of how painfully bored he is and he doesnt even know why hes doing it hes a pretty logical guy. he will go to class and just sit there for and hour most of the time doing nothing challenging because thats the way our school works. he has clinically diagnosed depression, has been to a psychiatric ward, and sees a therapist regularly. its only been a couple hours since he showed me his arm, and i didnt even see all of it. ive got two main problems right now. first i dont really know how to help him. he said literally the only thing i can really do is just hang out with him after school and on the weekends so he doesnt just sit in bed all day. ive been trying to make an effort to do that as much as i can but i need to do more. he said himself that as long as he goes to highschool, and considering its the law, hes going to feel this way until we graduate. i just need to know something i can do to help him, do i tell his parents? a therapist wont help he already goes to one. should i talk to him more about it tomorrow. i just dont knowsecond, stuff like this is really effecting me. i just feel that the world is being unfair to everyone i care about and i just want everyone i love to feel better. as of lately ive been feeling depressed whenever my friends need to talk about issues and its really weighing down on me. i just dont want to leave the people i care about down in the dirt. so yeah if anyone has any advice whatsoever please help me. im normally able to deal with these feelings and help my friends but right now im struggling. thank you '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 35378,  6661,     2,     1,     1],\n",
       "        [    0,  1274,  3642,   621,   398,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = tokenizer(['Hello World', 'hi how are you'], padding=True, truncation=True, return_tensors='pt')\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2215, -0.3197],\n",
       "        [-0.2117, -0.3234]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "example['input_ids'] = example['input_ids'].type(torch.LongTensor) \n",
    "testing = model(**example)\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sabrinalopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1516\n",
      "Validation set size: 189\n",
      "Test set size: 190\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_texts = df['text'].tolist()\n",
    "data_labels = df['label'].tolist()\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(data_texts, data_labels, test_size=0.2, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Train set size:\", len(train_texts))\n",
    "print(\"Validation set size:\", len(val_texts))\n",
    "print(\"Test set size:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SDCNLDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SDCNLDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = SDCNLDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = SDCNLDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SabrinaLopez\\anaconda3\\envs\\nlpproject\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\SabrinaLopez\\AppData\\Local\\Temp\\ipykernel_1512\\3545177458.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda p: {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "        \"precision\": precision_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary'),\n",
    "        \"recall\": recall_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary'),\n",
    "        \"f1\": f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average='binary'),\n",
    "        \"auroc\": roc_auc_score(p.label_ids, p.predictions[:, 1])\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "                                                \n",
      "\n",
      "\u001b[A\u001b[A                                          \n",
      "\u001b[A                                            \n",
      "\n",
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 95/285 [14:13<13:33,  4.28s/it]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6389954686164856, 'eval_accuracy': 0.6931216931216931, 'eval_precision': 0.7280701754385965, 'eval_recall': 0.7545454545454545, 'eval_f1': 0.7410714285714286, 'eval_auroc': 0.728078250863061, 'eval_runtime': 9.5536, 'eval_samples_per_second': 19.783, 'eval_steps_per_second': 1.256, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                      \n",
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 95/285 [20:40<13:33,  4.28s/it] \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6752640008926392, 'eval_accuracy': 0.7248677248677249, 'eval_precision': 0.7685185185185185, 'eval_recall': 0.7545454545454545, 'eval_f1': 0.7614678899082569, 'eval_auroc': 0.7445339470655926, 'eval_runtime': 9.4503, 'eval_samples_per_second': 19.999, 'eval_steps_per_second': 1.27, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "                                                \n",
      "\u001b[A                                            \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A                                      \n",
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 95/285 [27:10<13:33,  4.28s/it] \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7225422263145447, 'eval_accuracy': 0.6931216931216931, 'eval_precision': 0.7549019607843137, 'eval_recall': 0.7, 'eval_f1': 0.7264150943396226, 'eval_auroc': 0.7376294591484464, 'eval_runtime': 12.9497, 'eval_samples_per_second': 14.595, 'eval_steps_per_second': 0.927, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      "\u001b[A                                            \n",
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 95/285 [27:20<13:33,  4.28s/it] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 285/285 [19:28<00:00,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1168.3654, 'train_samples_per_second': 3.893, 'train_steps_per_second': 0.244, 'train_loss': 0.4883400900322094, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=285, training_loss=0.4883400900322094, metrics={'train_runtime': 1168.3654, 'train_samples_per_second': 3.893, 'train_steps_per_second': 0.244, 'total_flos': 299157269944320.0, 'train_loss': 0.4883400900322094, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.7225422263145447, 'eval_accuracy': 0.6931216931216931, 'eval_precision': 0.7549019607843137, 'eval_recall': 0.7, 'eval_f1': 0.7264150943396226, 'eval_auroc': 0.7376294591484464, 'eval_runtime': 12.3178, 'eval_samples_per_second': 15.344, 'eval_steps_per_second': 0.974, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:11<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'test_loss': 0.6350271701812744, 'test_accuracy': 0.7052631578947368, 'test_precision': 0.775, 'test_recall': 0.62, 'test_f1': 0.6888888888888889, 'test_auroc': 0.7912222222222222, 'test_runtime': 12.5336, 'test_samples_per_second': 15.159, 'test_steps_per_second': 0.957}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.predict(test_dataset)\n",
    "print(\"Test Results:\", test_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
