{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas pyarrow fastparquet huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"hf://datasets/KAIST-IC-LAB721/SDCNL/data/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, str in enumerate(df['text']):\n",
    "    # print(string)\n",
    "\n",
    "    clean_str = list([val for val in str if val.isalnum() or val == ' '])\n",
    "    clean_str = ''.join(clean_str)\n",
    "    low_clean_str = clean_str.lower()\n",
    "\n",
    "    # print(low_clean_str)\n",
    "\n",
    "    df = df.replace(df['text'][idx], low_clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = XLMRobertaModel.from_pretrained('xlm-roberta-base', num_labels=2, hidden_dropout_prob=0.3,    # Add dropout\n",
    "    attention_probs_dropout_prob=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: False\n"
     ]
    }
   ],
   "source": [
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(f\"Current device is: {device}\")  # Will print \"cuda\" if using CUDA\n",
    "print(f\"Is CUDA?: {device.type == 'cuda'}\")  # Will print True if using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Remove the str() conversion since texts are already strings\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypeparameters for training need to be modified to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=15):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.1)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_train_labels = []\n",
    "        all_train_predictions = []\n",
    "        all_train_probs = []\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "            all_train_predictions.extend(predictions.cpu().numpy())\n",
    "            all_train_probs.extend(torch.softmax(outputs.logits, dim=1)[:, 1].detach().cpu().numpy())  # Detach tensor\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{(correct/total)*100:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_acc = accuracy_score(all_train_labels, all_train_predictions) * 100\n",
    "        train_precision = precision_score(all_train_labels, all_train_predictions)\n",
    "        train_recall = recall_score(all_train_labels, all_train_predictions)\n",
    "        train_f1 = f1_score(all_train_labels, all_train_predictions)\n",
    "        train_auroc = roc_auc_score(all_train_labels, all_train_probs)\n",
    "        \n",
    "        print(f'Training Accuracy: {train_acc:.2f}%')\n",
    "        print(f'Training Precision: {train_precision:.4f}')\n",
    "        print(f'Training Recall: {train_recall:.4f}')\n",
    "        print(f'Training F1-score: {train_f1:.4f}')\n",
    "        print(f'Training AUROC: {train_auroc:.4f}')\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_val_labels = []\n",
    "        all_val_predictions = []\n",
    "        all_val_probs = []\n",
    "        \n",
    "        print(\"\\nRunning validation...\")\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                probs = torch.softmax(outputs.logits, dim=1)[:, 1]\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "                all_val_predictions.extend(predictions.cpu().numpy())\n",
    "                all_val_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        val_acc = (val_correct/val_total)*100\n",
    "        val_precision = precision_score(all_val_labels, all_val_predictions)\n",
    "        val_recall = recall_score(all_val_labels, all_val_predictions)\n",
    "        val_f1 = f1_score(all_val_labels, all_val_predictions)\n",
    "        val_auroc = roc_auc_score(all_val_labels, all_val_probs)\n",
    "\n",
    "        print(f'Validation Accuracy: {val_acc:.2f}%')\n",
    "        print(f'Validation Precision: {val_precision:.4f}')\n",
    "        print(f'Validation Recall: {val_recall:.4f}')\n",
    "        print(f'Validation F1-score: {val_f1:.4f}')\n",
    "        print(f'Validation AUROC: {val_auroc:.4f}')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'Saved new best model with validation accuracy: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model with proper configuration\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    'xlm-roberta-base',\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\"\n",
    ").to(device)\n",
    "\n",
    "# Split data - making sure to convert to list\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    list(df['text']),\n",
    "    list(df['label']),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts,\n",
    "    temp_labels,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextClassificationDataset(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = TextClassificationDataset(\n",
    "    val_texts,\n",
    "    val_labels,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Create dataloaders with smaller batch size for CPU\n",
    "batch_size = 4 if device == 'cpu' else 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4  # Set to 0 for CPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4  # Set to 0 for CPU\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    train_model(model, train_loader, val_loader, device)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(texts, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    encoded_texts = tokenizer(\n",
    "        texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoded_texts['input_ids'].to(device)\n",
    "    attention_mask = encoded_texts['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "    \n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Test Accuracy: {acc:.2f}\")\n",
    "    \n",
    "\n",
    "test_dataset = TextClassificationDataset(\n",
    "    test_texts,\n",
    "    test_labels,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4  # Set to 0 for CPU\n",
    ")\n",
    "\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "model.to(device)\n",
    "\n",
    "# Example prediction\n",
    "new_texts = [test_dataset.texts[0], test_dataset.texts[1]]\n",
    "predictions = [predict(test_dataset.texts[0], model, tokenizer, device), predict(test_dataset.texts[0], model, tokenizer, device)]\n",
    "print(f'Sample text: {new_texts}\\n True labels: {test_dataset.labels[0], test_dataset.labels[1]}\\n Predictions: {predictions}')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "        probs = torch.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy()\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted_labels)\n",
    "        all_probs.extend(probs)\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_predictions) * 100\n",
    "test_precision = precision_score(all_labels, all_predictions)\n",
    "test_recall = recall_score(all_labels, all_predictions)\n",
    "test_f1 = f1_score(all_labels, all_predictions)\n",
    "test_auroc = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "print(f'Test Precision: {test_precision:.4f}')\n",
    "print(f'Test Recall: {test_recall:.4f}')\n",
    "print(f'Test F1-score: {test_f1:.4f}')\n",
    "print(f'Test AUROC: {test_auroc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to fix up the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap, transformers, numpy as np, matplotlib as plt, pandas as pd\n",
    "\n",
    "pred = transformers.pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer(\n",
    "        test_texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    ),\n",
    "    device=device,\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "shap_explainer = shap.Explainer(pred)\n",
    "shap_values = shap_explainer(test_texts)\n",
    "\n",
    "shap.initjs()\n",
    "shap.plots.bar(shap_values[:, :, 0].mean(0), order=shap.Explanation.argsort)\n",
    "shap.plots.bar(shap_values[:, :, 1].mean(0), order=shap.Explanation.argsort)\n",
    "figure = plt.figure()\n",
    "shap.summary_plot(shap_values, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=\"english\")\n",
    "X_test = vectorizer.fit_transform(test_texts)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Feature Names:\", feature_names)\n",
    "\n",
    "labels = np.array([1, 0])  \n",
    "lime_explainer = LimeTabularExplainer(\n",
    "    training_data=X_test.toarray(),  # Convert sparse matrix to dense array\n",
    "    feature_names=feature_names,\n",
    "    class_names=[0, 1], \n",
    "    verbose=True,\n",
    "    mode=\"classification\"\n",
    ")\n",
    "\n",
    "# Testing single instance\n",
    "lime_explanation = lime_explainer.explain_instance(\n",
    "    X_test.toarray()[0],\n",
    "    model.predict_proba,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "lime_explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LIME for multiple instances\n",
    "for i in range(len(X_test.toarray())):\n",
    "    explanation = lime_explainer.explain_instance(\n",
    "        X_test.toarray()[i],\n",
    "        model.predict_proba,\n",
    "        num_features=10\n",
    "    )\n",
    "\n",
    "    explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_feature_importance = shap_values.values\n",
    "shap_feature_names = shap_values.feature_names\n",
    "\n",
    "lime_feature_importance = {name: weight for name, weight in lime_explanation.as_list()}\n",
    "\n",
    "shap_norm = np.abs(shap_feature_importance / np.sum(np.abs(shap_feature_importance)))\n",
    "lime_norm = np.array([lime_feature_importance.get(name, 0) for name in shap_feature_names])\n",
    "lime_norm = np.abs(lime_norm / np.sum(np.abs(lime_norm)))\n",
    "\n",
    "combined_importance = pd.DataFrame({\n",
    "    \"Feature\": shap_feature_names,\n",
    "    \"SHAP Importance\": shap_norm,\n",
    "    \"LIME Importance\": lime_norm\n",
    "})\n",
    "\n",
    "combined_importance = combined_importance.sort_values(by=\"SHAP Importance\", ascending=False)\n",
    "\n",
    "combined_importance.set_index(\"Feature\").plot(kind=\"bar\", figsize=(10, 6))\n",
    "plt.title(\"SHAP vs LIME Feature Importance\")\n",
    "plt.ylabel(\"Normalized Importance\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
