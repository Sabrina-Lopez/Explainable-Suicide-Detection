{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (18.0.0)\n",
      "Requirement already satisfied: fastparquet in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (2024.11.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from fastparquet) (2.9.0)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from fastparquet) (2024.10.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from fastparquet) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/KAIST-IC-LAB721/SDCNL/data/train-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Need help</td>\n",
       "      <td>Hi I don't really know how to phrase this situ...</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feeling so overwhelmed and hopeless</td>\n",
       "      <td>i have been so depressed these past couple wee...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicidal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nothing matters anymore, getting worse</td>\n",
       "      <td>Hi..I don't know where else to go. I am devast...</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who’s tired of hearing bullshit</td>\n",
       "      <td>The shit like “it will get better, everyone is...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicidal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I wish I was someone else.</td>\n",
       "      <td>I wish I was prettier. I wish I didn’t feel li...</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>think its over</td>\n",
       "      <td>i just don’t wanna live anymore so yeah</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>To all of those feeling isolated and suffering...</td>\n",
       "      <td>I’ve learned that life is fucking sad sometime...</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>I just really wish I had died the first time I...</td>\n",
       "      <td>That's all. Nothing has gotten better and I've...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicidal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>I feel unimportant.</td>\n",
       "      <td>Not the first time I'm going through this of c...</td>\n",
       "      <td>0</td>\n",
       "      <td>depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>I’m now genuinely thinking life for others wou...</td>\n",
       "      <td>Fight with my fiancée. Potential for her endin...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicidal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1895 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                                             Need help   \n",
       "1                   feeling so overwhelmed and hopeless   \n",
       "2                Nothing matters anymore, getting worse   \n",
       "3                       Who’s tired of hearing bullshit   \n",
       "4                            I wish I was someone else.   \n",
       "...                                                 ...   \n",
       "1890                                     think its over   \n",
       "1891  To all of those feeling isolated and suffering...   \n",
       "1892  I just really wish I had died the first time I...   \n",
       "1893                                I feel unimportant.   \n",
       "1894  I’m now genuinely thinking life for others wou...   \n",
       "\n",
       "                                                   text  label  label_text  \n",
       "0     Hi I don't really know how to phrase this situ...      0  depression  \n",
       "1     i have been so depressed these past couple wee...      1    suicidal  \n",
       "2     Hi..I don't know where else to go. I am devast...      0  depression  \n",
       "3     The shit like “it will get better, everyone is...      1    suicidal  \n",
       "4     I wish I was prettier. I wish I didn’t feel li...      0  depression  \n",
       "...                                                 ...    ...         ...  \n",
       "1890            i just don’t wanna live anymore so yeah      0  depression  \n",
       "1891  I’ve learned that life is fucking sad sometime...      0  depression  \n",
       "1892  That's all. Nothing has gotten better and I've...      1    suicidal  \n",
       "1893  Not the first time I'm going through this of c...      0  depression  \n",
       "1894  Fight with my fiancée. Potential for her endin...      1    suicidal  \n",
       "\n",
       "[1895 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Hi I don't really know how to phrase this situ...\n",
       "1    i have been so depressed these past couple wee...\n",
       "2    Hi..I don't know where else to go. I am devast...\n",
       "3    The shit like “it will get better, everyone is...\n",
       "4    I wish I was prettier. I wish I didn’t feel li...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, str in enumerate(df['text']):\n",
    "    # print(string)\n",
    "\n",
    "    clean_str = list([val for val in str if val.isalnum() or val == ' '])\n",
    "    clean_str = ''.join(clean_str)\n",
    "    low_clean_str = clean_str.lower()\n",
    "\n",
    "    # print(low_clean_str)\n",
    "\n",
    "    df = df.replace(df['text'][idx], low_clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi i dont really know how to phrase this situation but ill try my life is at a really good point right now im never really depressed over stuff and 99 percent of the time my mind is clear im about to graduate high school and im really excited however people in my family and friend group have tons of issues wether they sleep all day hate themselves or have no ambition to keep living on in this world theyve got problems i dont wanna sound like im gloating but usually im the person that a lot of these people end up going to because usually im able to talk people through issues and help them in the long run yeah sometimes their issues make me really sad and stuff because who doesnt feel sad when people are telling you they feel worthlessbut today one of my best friends showed me that he was cutting and it really effected me i talked to him about it and its mostly because of how painfully bored he is and he doesnt even know why hes doing it hes a pretty logical guy he will go to class and just sit there for and hour most of the time doing nothing challenging because thats the way our school works he has clinically diagnosed depression has been to a psychiatric ward and sees a therapist regularly its only been a couple hours since he showed me his arm and i didnt even see all of it ive got two main problems right now first i dont really know how to help him he said literally the only thing i can really do is just hang out with him after school and on the weekends so he doesnt just sit in bed all day ive been trying to make an effort to do that as much as i can but i need to do more he said himself that as long as he goes to highschool and considering its the law hes going to feel this way until we graduate i just need to know something i can do to help him do i tell his parents a therapist wont help he already goes to one should i talk to him more about it tomorrow i just dont knowsecond stuff like this is really effecting me i just feel that the world is being unfair to everyone i care about and i just want everyone i love to feel better as of lately ive been feeling depressed whenever my friends need to talk about issues and its really weighing down on me i just dont want to leave the people i care about down in the dirt so yeah if anyone has any advice whatsoever please help me im normally able to deal with these feelings and help my friends but right now im struggling thank you '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (4.46.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = XLMRobertaModel.from_pretrained('xlm-roberta-base', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/nlp/lib/python3.9/site-packages (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install  tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import XLMRobertaForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"MPS available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Remove the str() conversion since texts are already strings\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=3):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{(correct/total)*100:.2f}%'\n",
    "            })\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        print(\"\\nRunning validation...\")\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_acc = (val_correct/val_total)*100\n",
    "        print(f'Validation Accuracy: {val_acc:.2f}%')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "            print(f'Saved new best model with validation accuracy: {val_acc:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model with proper configuration\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "        'xlm-roberta-base',\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Split data - making sure to convert to list\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        list(df['text']),\n",
    "        list(df['label']),\n",
    "        test_size=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextClassificationDataset(\n",
    "        train_texts,\n",
    "        train_labels,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    val_dataset = TextClassificationDataset(\n",
    "        val_texts,\n",
    "        val_labels,\n",
    "        tokenizer\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders with smaller batch size for CPU\n",
    "    batch_size = 4 if device == 'cpu' else 8\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0  # Set to 0 for CPU\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0  # Set to 0 for CPU\n",
    "    )\n",
    "    \n",
    "    # Print dataset sizes\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        train_model(model, train_loader, val_loader, device)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training samples: 1705\n",
      "Validation samples: 190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1590f3a493c441a99a67401c30db091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
